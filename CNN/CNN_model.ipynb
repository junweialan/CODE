{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras import models, layers, losses, optimizers, activations, regularizers\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from  tensorflow.keras.layers import Input, BatchNormalization, Conv1D, MaxPooling1D, Dropout\n",
    "from  tensorflow.keras.layers import LeakyReLU\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model:\n",
    "\n",
    "    def __init__(self, num_samples, input_volume,filters = 5, kernel_size = 5, pool_size = 2 ,use_bias = False):\n",
    "        \n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', name='conv_1', use_bias=use_bias, input_shape=(num_samples, input_volume)))\n",
    "        self.model.add(BatchNormalization(name='norm_1'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_1'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_1'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', name='conv_2', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_2'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_2'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_2'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', name='conv_3', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_3'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_3'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_3'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same',name='conv_4', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name = 'norm_4'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_4'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_4'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', name='conv_5', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_5'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_5'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_5'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', name='conv_6', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_6'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_6'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_6'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same',name='conv_7', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name = 'norm_7'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_7'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_7'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', name='conv_8', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_8'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_8'))\n",
    "        self.model.add(MaxPooling1D(pool_size=pool_size, name='pool_8'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=filters, kernel_size=kernel_size, padding='same', name='conv_9', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_9'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_9'))\n",
    "        \n",
    "        self.model.add(layers.Flatten())\n",
    "\n",
    "        self.model.add(layers.Dense(units=200, activation=activations.linear, name='dense_5', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_dense_5'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_dense_5'))\n",
    "\n",
    "        self.model.add(layers.Dense(units=50, activation=activations.linear, name='dense_6', use_bias=use_bias))\n",
    "        self.model.add(BatchNormalization(name='norm_dense_6'))\n",
    "        self.model.add(LeakyReLU(alpha=0.1, name='leaky_relu_dense_6'))\n",
    "\n",
    "        self.model.add(layers.Dense(units=2, activation=activations.linear, name='dense_7', use_bias=use_bias))\n",
    "\n",
    "    def get_model(self, learning_rate=0.001, decay=0.0001):\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=learning_rate,\n",
    "            decay_steps=5850,  \n",
    "            decay_rate=decay\n",
    "        )\n",
    "        self.model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule), loss=losses.mean_squared_error)\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "    def normalized_mean_squared_error(self):\n",
    "        err_pmus = np.array(self.y_true)\n",
    "        err_pmus_hat = np.array(self.y_pred)\n",
    "        nmse = np.sqrt(np.sum((err_pmus - err_pmus_hat)**2))/np.sqrt(np.sum((err_pmus - np.average(err_pmus))**2))\n",
    "        return nmse\n",
    "    \n",
    "    def r_square(self):\n",
    "        r2 = r2_score(self.y_true, self.y_pred)\n",
    "        return r2\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        mse = mean_squared_error(self.y_true, self.y_pred)\n",
    "        mae = mean_absolute_error(self.y_true, self.y_pred)\n",
    "        nmse = self.normalized_mean_squared_error()\n",
    "        r2 = self.r_square()\n",
    "        mape = mean_absolute_percentage_error(self.y_true, self.y_pred)*100\n",
    "\n",
    "        return mse, mae, nmse, mape, r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlandAltman:\n",
    "    def __init__(self, output_data_unscaled, pre_data_unscaled):\n",
    "        self.output_data_unscaled = output_data_unscaled\n",
    "        self.pre_data_unscaled = pre_data_unscaled\n",
    "\n",
    "    def plot(self, x_label, start, xlim_range,start_y,ylim_range):\n",
    "        # Convert the values to numpy arrays\n",
    "        output_data = np.asarray(self.output_data_unscaled)\n",
    "        pre_data = np.asarray(self.pre_data_unscaled)\n",
    "\n",
    "        # Calculate the difference and mean\n",
    "        diff = output_data - pre_data\n",
    "        mean = (output_data + pre_data) / 2\n",
    "\n",
    "        # Calculate the percentage difference between predictions and targets\n",
    "        percentage_diff = (diff / mean) * 100\n",
    "\n",
    "        # Calculate the mean and standard deviation of the percentage difference\n",
    "        mean_diff = np.mean(percentage_diff)\n",
    "        std_diff = np.std(percentage_diff)\n",
    "\n",
    "        overestimate_indices = []\n",
    "        underestimate_indices = []\n",
    "        # Find indices of outliers\n",
    "        outlier_indices = []\n",
    "\n",
    "        for i, percentage_error in enumerate(percentage_diff):\n",
    "            if percentage_error > 20:  # overestimate\n",
    "                outlier_indices.append(i)\n",
    "                overestimate_indices.append(i)\n",
    "            elif percentage_error < -20:  # underestimate\n",
    "                outlier_indices.append(i)\n",
    "                underestimate_indices.append(i)\n",
    "\n",
    "        # Plot the Bland-Altman plot with percentage difference\n",
    "        fig, ax = plt.subplots(figsize=(8, 5.5))\n",
    "        for i in range(len(percentage_diff)):\n",
    "            if i in outlier_indices:\n",
    "                ax.scatter(output_data[i], percentage_diff[i], color='red', s=15)\n",
    "            else:\n",
    "                ax.scatter(output_data[i], percentage_diff[i], color='blue', s=15)\n",
    "        ax.axhline(mean_diff, color='gray', linestyle='--',lw = 2)\n",
    "        ax.axhline(mean_diff + 1.96 * std_diff, color='gray', linestyle='--',lw = 2)\n",
    "        ax.axhline(mean_diff - 1.96 * std_diff, color='gray', linestyle='--',lw = 2)\n",
    "        ax.axhline(20, color='green', linestyle='-',lw = 2.5)\n",
    "        ax.axhline(-20, color='green', linestyle='-',lw = 2.5)\n",
    "        ax.set_xlabel(x_label, fontsize=14)\n",
    "        ax.set_ylabel('Percentage Difference between Predictions and Targets', fontsize=11)\n",
    "        ax.set_title('Bland-Altman Plot', fontsize=16)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        ax.set_xlim(start, xlim_range)\n",
    "        ax.set_ylim(start_y, ylim_range)\n",
    "        #yticks = np.arange(start_y, ylim_range + 1, 10)  \n",
    "        #plt.yticks(yticks)\n",
    "        ax.set_yticks([-40,-20, 0, 20, 40])\n",
    "        plt.show()\n",
    "\n",
    "        # Print indices of outliers\n",
    "        print(\"Indices of outliers: \", outlier_indices)\n",
    "        print(\"Indices of overestimations: \", overestimate_indices)\n",
    "        print(\"Indices of underestimations: \", underestimate_indices)\n",
    "        return outlier_indices, overestimate_indices, underestimate_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, minimum = None,maximum = None):\n",
    "    if minimum is None:\n",
    "        minimum = np.min(np.min(data))\n",
    "    if maximum is None:\n",
    "        maximum = np.max(np.max(data))\n",
    "    data_norm = (data - minimum) / (maximum - minimum)\n",
    "    return minimum, maximum, data_norm\n",
    "\n",
    "\n",
    "def denormalize_data(data, minimum, maximum):\n",
    "    return data * (maximum - minimum) + minimum\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Junwei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
