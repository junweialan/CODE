{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import stft\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalProcessor:\n",
    "    def __init__(self, flow, sampling_rate):\n",
    "        self.flow = flow\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.downsampled_signals = self.downsample_signals()\n",
    "        self.delta_seqs = self.calculate_delta_sequences()\n",
    "\n",
    "    def downsample_signals(self):\n",
    "        downsampled_signals = []\n",
    "        for i in range(self.flow.shape[0]):\n",
    "            signal = self.flow[i] \n",
    "\n",
    "            # Downsample the signal to the specified sampling rate\n",
    "            downsampled_signal = signal[::self.sampling_rate]\n",
    "\n",
    "            # Append the downsampled signal to list\n",
    "            downsampled_signals.append(downsampled_signal)\n",
    "\n",
    "        return downsampled_signals\n",
    "\n",
    "    def calculate_delta_sequences(self):\n",
    "        delta_seqs = []\n",
    "        for signal in self.downsampled_signals:\n",
    "            previous = signal[0]\n",
    "            delta_seq = []\n",
    "            for current in signal[1:]:\n",
    "                delta = current - previous\n",
    "                delta_seq.append(delta)\n",
    "                previous = current\n",
    "            delta_seqs.append(delta_seq)\n",
    "\n",
    "        return delta_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianQuantizer:\n",
    "    def __init__(self, data, num_bins, alphabet):\n",
    "        self.data = data\n",
    "        self.num_bins = num_bins\n",
    "        self.alphabet = alphabet\n",
    "        self.mean, self.std = norm.fit(data)\n",
    "        self.bin_edges = self.calculate_bin_edges()\n",
    "\n",
    "    def calculate_bin_edges(self):\n",
    "        hist_values, bin_edges = np.histogram(self.data, bins=self.num_bins, density=True)\n",
    "        return bin_edges\n",
    "\n",
    "    def value_to_alphabet(self, value):\n",
    "        for i in range(len(self.bin_edges) - 1):\n",
    "            if self.bin_edges[i] <= value < self.bin_edges[i+1]:\n",
    "                return self.alphabet[i]\n",
    "        return self.alphabet[-1]\n",
    "\n",
    "    def convert_dataset(self, dataset):\n",
    "        converted_dataset = []\n",
    "        for row in dataset:\n",
    "            converted_row = [self.value_to_alphabet(value) for value in row]\n",
    "            converted_dataset.append(converted_row)\n",
    "        return converted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDic1:\n",
    "    def __init__(self, f, p, v):\n",
    "        self.f = f\n",
    "        self.p = p\n",
    "        self.v = v\n",
    "        self.dataset = self.create_dataset()\n",
    "        self.words_dataset = self.create_words_dataset()\n",
    "        self.word_to_index = self.create_word_to_index()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        concatenated_samples = []\n",
    "        for i in range(self.v.shape[0]):\n",
    "            concatenated_sample = np.concatenate([self.f[i], self.p[i], self.v[i]])\n",
    "            concatenated_samples.append(concatenated_sample)\n",
    "\n",
    "        # Convert the list of concatenated samples to a numpy\n",
    "        dataset = np.array(concatenated_samples)\n",
    "        return dataset\n",
    "\n",
    "    def create_words_dataset(self):\n",
    "        words_dataset = []\n",
    "\n",
    "        # Iterate through the samples and create words of length 3\n",
    "        for sample in self.dataset:\n",
    "            words = [''.join(sample[i:i+3]) for i in range(0, len(sample), 3)]\n",
    "            words_dataset.append(words)\n",
    "        return words_dataset\n",
    "\n",
    "    def create_word_to_index(self):\n",
    "        unique_dataset_words = np.unique(self.words_dataset)\n",
    "        word_to_index = {word: index for index, word in enumerate(unique_dataset_words)}\n",
    "\n",
    "        mapping_function = lambda x: word_to_index[x] if x in word_to_index else x\n",
    "        mapped_dataset = np.vectorize(mapping_function)(self.words_dataset)\n",
    "        \n",
    "        return word_to_index, mapped_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class downsamplePlotter:\n",
    "    def __init__(self, pressure_signal,downsampling_rate):\n",
    "        self.pressure_signal = pressure_signal\n",
    "        self.downsampling_rate = downsampling_rate\n",
    "        self.x_coord = np.arange(0, pressure_signal.size, self.downsampling_rate)\n",
    "\n",
    "    def plot_pressure_signal(self):\n",
    "        pressure_0 = self.pressure_signal\n",
    "        fig, (ax1, ax3) = plt.subplots(nrows=2,figsize=(10, 8))\n",
    "\n",
    "        ax1.set_title('Original Pressure Signal')\n",
    "        ax1.plot(pressure_0,label='Original Signal')\n",
    "        ax1.set_ylabel('Pressure')\n",
    "        ax1.set_xlabel('Time')\n",
    "\n",
    "        #ax2.set_title('Sampled signal')\n",
    "        #ax2.plot(pressure_0)\n",
    "        #for x in np.nditer(self.x_coord):\n",
    "         #   ax2.axvline(x, color='red', linewidth=0.5)\n",
    "        #ax2.set_ylabel('Pressure')\n",
    "        #ax2.set_xlabel('Time')\n",
    "        ax3.set_title('Downsampled Signal')\n",
    "        ax3.plot(range(0, pressure_0.size, self.downsampling_rate), pressure_0[::self.downsampling_rate], 'ro', markersize=3, color=\"r\")\n",
    "        ax3.plot(range(0, pressure_0.size, self.downsampling_rate), pressure_0[::self.downsampling_rate], 'b-', linewidth=0.5, color='blue')\n",
    "        ax3.set_ylabel('Pressure')\n",
    "        ax3.set_xlabel('Time')\n",
    "\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gaussian_histogram:\n",
    "    def __init__(self, data,num_bins,num_std):\n",
    "        self.data = data\n",
    "        self.num_bins = num_bins\n",
    "        self.num_std = num_std\n",
    "\n",
    "    def plot_gaussian_histogram(self):\n",
    "       \n",
    "        plt.hist(self.data, bins=self.num_bins, edgecolor='black', density=True)\n",
    "\n",
    "        mean, std = norm.fit(self.data)\n",
    "\n",
    "        x = np.linspace(mean - self.num_std*std, mean + self.num_std*std)\n",
    "\n",
    "        y = norm.pdf(x, mean, std)\n",
    "        plt.plot(x, y)\n",
    "\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Gaussian Distribution - All Signals')\n",
    "        #plt.savefig(output_filename, dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDic2:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def create_words_dataset(self):\n",
    "        words_dataset = []\n",
    "        for sample in self.dataset:\n",
    "            words = [''.join(sample[i:i+3]) for i in range(0, len(sample), 3)]\n",
    "            words_dataset.append(words)\n",
    "        self.words_dataset = np.array(words_dataset)\n",
    "        return self.words_dataset\n",
    "    \n",
    "    def create_word_to_index(self, start_index=0):\n",
    "        unique_dataset_words = np.unique(self.words_dataset)\n",
    "        word_to_index = {word: index + start_index for index, word in enumerate(unique_dataset_words)}\n",
    "        mapping_function = lambda x: word_to_index[x] if x in word_to_index else x\n",
    "        mapped_dataset = np.vectorize(mapping_function)(self.words_dataset)\n",
    "        return word_to_index, mapped_dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
