{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import stft\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class ExcelDataReader:\n",
    " #   def __init__(self, path):\n",
    "  #      self.path = path\n",
    "   #     self.files = sorted(glob.glob(path + '\\\\*baseline.xlsx'), key=len)\n",
    "    #    self.cols_to_keep = [0, 1, 3]\n",
    "     #   self.data = None\n",
    "    \n",
    "   # def read_and_process_data(self, num_rows=1000):\n",
    "    #    dfs = []\n",
    "#\n",
    " #       for file in self.files:\n",
    "  #          df = pd.read_excel(file, sheet_name='FrontSheet')\n",
    "   #         df = df.iloc[:num_rows, self.cols_to_keep]\n",
    "#\n",
    " #           df = df.to_numpy()\n",
    "  #          dfs.append(df)\n",
    "#\n",
    " #       self.data = np.stack(dfs, axis=0)\n",
    "  #      \n",
    "   #     return self.data, self.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class ExcelDataReader:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.files = sorted(glob.glob(path + '\\\\*baseline.xlsx'), key=lambda x: int(re.findall(r'\\d+', x)[-1]))\n",
    "        self.cols_to_keep = [0, 1, 3]\n",
    "        self.data = None\n",
    "    \n",
    "    def read_and_process_data(self, num_rows=1000):\n",
    "        dfs = []\n",
    "\n",
    "        for file in self.files:\n",
    "            df = pd.read_excel(file, sheet_name='FrontSheet')\n",
    "            df = df.iloc[:num_rows, self.cols_to_keep]\n",
    "\n",
    "            df = df.to_numpy()\n",
    "            dfs.append(df)\n",
    "\n",
    "        self.data = np.stack(dfs, axis=0)\n",
    "        \n",
    "        return self.data, self.files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_numbers_after_mv(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    pattern = re.compile(r'MV(\\d+)')\n",
    "\n",
    "    # Iterate over the files and find the numbers after \"MV\"\n",
    "    numbers = []\n",
    "    for file in files:\n",
    "        match = pattern.search(file)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "            numbers.append(number)\n",
    "    # Remove duplicates by converting the list to a set and then back to a list\n",
    "    unique_numbers = list(set(numbers))\n",
    "\n",
    "    return unique_numbers\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MV001-100 AND Find out missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "#reader = ExcelDataReader(r'F:\\junwei\\dataset\\MV001-100')\n",
    "\n",
    "#data,files = reader.read_and_process_data(num_rows=1000)\n",
    "#print(data.shape)\n",
    "\n",
    "#data = data.transpose((0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files: {'MV56baseline.xlsx', 'MV61baseline.xlsx', 'MV41baseline.xlsx', 'MV60baseline.xlsx', 'MV33baseline.xlsx', 'MV74baseline.xlsx', 'MV24baseline.xlsx', 'MV93baseline.xlsx', 'MV19baseline.xlsx', 'MV48baseline.xlsx', 'MV81baseline.xlsx', 'MV30baseline.xlsx'}\n"
     ]
    }
   ],
   "source": [
    "#path = r'F:\\junwei\\dataset\\MV001-100'\n",
    "#expected_filenames = [f'MV{i}baseline.xlsx' for i in range(1, 100)]\n",
    "\n",
    "#actual_filenames = []\n",
    "#for file in glob.glob(os.path.join(path, '*baseline.xlsx')):\n",
    " #   actual_filenames.append(os.path.basename(file))\n",
    "\n",
    "#missing_files1 = set(expected_filenames) - set(actual_filenames)\n",
    "#print(f\"Missing files: {missing_files1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MV101-200 AND Find out missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "#reader2 = ExcelDataReader(r'F:\\junwei\\dataset\\MV101-200')\n",
    "\n",
    "#data2,files2 = reader2.read_and_process_data(num_rows=1000)\n",
    "\n",
    "#data2 = data2.transpose((0, 2, 1))\n",
    "\n",
    "#print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files: {'MV123baseline.xlsx', 'MV174baseline.xlsx', 'MV101baseline.xlsx', 'MV108baseline.xlsx', 'MV138baseline.xlsx', 'MV131baseline.xlsx', 'MV159baseline.xlsx', 'MV109baseline.xlsx', 'MV154baseline.xlsx', 'MV146baseline.xlsx', 'MV179baseline.xlsx', 'MV144baseline.xlsx', 'MV124baseline.xlsx', 'MV137baseline.xlsx', 'MV113baseline.xlsx', 'MV171baseline.xlsx'}\n"
     ]
    }
   ],
   "source": [
    "#path = r'F:\\junwei\\dataset\\MV101-200'\n",
    "#expected_filenames = [f'MV{i}baseline.xlsx' for i in range(101, 201)]\n",
    "\n",
    "#actual_filenames = []\n",
    "#for file in glob.glob(os.path.join(path, '*baseline.xlsx')):\n",
    " #   actual_filenames.append(os.path.basename(file))\n",
    "\n",
    "#missing_files2 = set(expected_filenames) - set(actual_filenames)\n",
    "#print(f\"Missing files: {missing_files2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MV201-300 AND Find out missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "#reader3 = ExcelDataReader(r'F:\\junwei\\dataset\\MV201-300')\n",
    "\n",
    "#data3,files3 = reader3.read_and_process_data(num_rows=1000)\n",
    "\n",
    "#data2 = data2.transpose((0, 2, 1))\n",
    "\n",
    "#print(data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files: {'MV293baseline.xlsx', 'MV288baseline.xlsx', 'MV267baseline.xlsx', 'MV206baseline.xlsx', 'MV229baseline.xlsx', 'MV254baseline.xlsx', 'MV233baseline.xlsx', 'MV263baseline.xlsx', 'MV261baseline.xlsx', 'MV213baseline.xlsx', 'MV250baseline.xlsx', 'MV258baseline.xlsx', 'MV255baseline.xlsx', 'MV209baseline.xlsx'}\n"
     ]
    }
   ],
   "source": [
    "#path = r'F:\\junwei\\dataset\\MV201-300'\n",
    "#expected_filenames = [f'MV{i}baseline.xlsx' for i in range(201, 301)]\n",
    "\n",
    "#actual_filenames = []\n",
    "#for file in glob.glob(os.path.join(path, '*baseline.xlsx')):\n",
    "#    actual_filenames.append(os.path.basename(file))\n",
    "\n",
    "#missing_files3 = set(expected_filenames) - set(actual_filenames)\n",
    "#print(f\"Missing files: {missing_files3}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MV301-400 AND Find out missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "#reader4 = ExcelDataReader(r'F:\\junwei\\dataset\\MV301-400')\n",
    "\n",
    "#data4,files4 = reader4.read_and_process_data(num_rows=1000)\n",
    "\n",
    "#data2 = data2.transpose((0, 2, 1))\n",
    "\n",
    "#print(data4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing files: {'MV381baseline.xlsx', 'MV332baseline.xlsx', 'MV390baseline.xlsx', 'MV336baseline.xlsx', 'MV394baseline.xlsx', 'MV398baseline.xlsx', 'MV391baseline.xlsx', 'MV321baseline.xlsx', 'MV383baseline.xlsx', 'MV400baseline.xlsx', 'MV315baseline.xlsx', 'MV397baseline.xlsx', 'MV386baseline.xlsx', 'MV396baseline.xlsx'}\n"
     ]
    }
   ],
   "source": [
    "#path = r'F:\\junwei\\dataset\\MV301-400'\n",
    "#expected_filenames = [f'MV{i}baseline.xlsx' for i in range(301, 401)]\n",
    "\n",
    "#actual_filenames = []\n",
    "#for file in glob.glob(os.path.join(path, '*baseline.xlsx')):\n",
    " #   actual_filenames.append(os.path.basename(file))\n",
    "\n",
    "#missing_files3 = set(expected_filenames) - set(actual_filenames)\n",
    "#print(f\"Missing files: {missing_files3}\")\n",
    "#381, 321"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read MV401-500 AND Find out missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 1000, 3)\n"
     ]
    }
   ],
   "source": [
    "#reader5 = ExcelDataReader(r'F:\\junwei\\dataset\\MV401-511')\n",
    "\n",
    "#data5,files5 = reader5.read_and_process_data(num_rows=1000)\n",
    "\n",
    "#data2 = data2.transpose((0, 2, 1))\n",
    "\n",
    "#print(data5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 1000, 3)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stacked_data = np.concatenate((data, data2, data3, data4, data5), axis=0)\n",
    "#stacked_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sex_1M2F</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Height</th>\n",
       "      <th>BMI</th>\n",
       "      <th>PredictedBW</th>\n",
       "      <th>APACHE</th>\n",
       "      <th>PF_ratio</th>\n",
       "      <th>SOFA_TOT</th>\n",
       "      <th>...</th>\n",
       "      <th>NonEFL_LeftFromParallelCriteria</th>\n",
       "      <th>NewEFL_type_parallel</th>\n",
       "      <th>Type2_removed</th>\n",
       "      <th>RelativeEFL_DeltaVt_Less20</th>\n",
       "      <th>NonEFL_LeftFromDeltaVtCriteria</th>\n",
       "      <th>NewEFL_type_deltaVt</th>\n",
       "      <th>DefiniteEFL_nonEFL</th>\n",
       "      <th>EFL_by_Standardized Criteria</th>\n",
       "      <th>EFL_by_NewlyProposed_RexAnalysis</th>\n",
       "      <th>EFL_by_mixed criteria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>65.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>23.875115</td>\n",
       "      <td>56.966</td>\n",
       "      <td>23.0</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>38.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>16.888889</td>\n",
       "      <td>43.316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>604.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>54.8</td>\n",
       "      <td>170.0</td>\n",
       "      <td>18.961938</td>\n",
       "      <td>66.016</td>\n",
       "      <td>19.0</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>84</td>\n",
       "      <td>46.9</td>\n",
       "      <td>150.0</td>\n",
       "      <td>20.844444</td>\n",
       "      <td>43.316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>61.7</td>\n",
       "      <td>160.0</td>\n",
       "      <td>24.101562</td>\n",
       "      <td>56.916</td>\n",
       "      <td>18.0</td>\n",
       "      <td>257.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>507</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>42.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>16.406250</td>\n",
       "      <td>56.916</td>\n",
       "      <td>17.0</td>\n",
       "      <td>326.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>508</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>44.1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>17.226562</td>\n",
       "      <td>56.916</td>\n",
       "      <td>11.0</td>\n",
       "      <td>546.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>509</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>52.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>19.333730</td>\n",
       "      <td>60.556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>85.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>28.731747</td>\n",
       "      <td>67.836</td>\n",
       "      <td>31.0</td>\n",
       "      <td>167.500000</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>60.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>24.034610</td>\n",
       "      <td>55.096</td>\n",
       "      <td>28.0</td>\n",
       "      <td>506.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Sex_1M2F  Age  Weight  Height        BMI  PredictedBW  APACHE  \\\n",
       "0      1         2   26    65.0   165.0  23.875115       56.966    23.0   \n",
       "1      2         2   82    38.0   150.0  16.888889       43.316     NaN   \n",
       "2      3         1   77    54.8   170.0  18.961938       66.016    19.0   \n",
       "3      4         2   84    46.9   150.0  20.844444       43.316     NaN   \n",
       "4      5         1   83    61.7   160.0  24.101562       56.916    18.0   \n",
       "..   ...       ...  ...     ...     ...        ...          ...     ...   \n",
       "451  507         1   79    42.0   160.0  16.406250       56.916    17.0   \n",
       "452  508         1   78    44.1   160.0  17.226562       56.916    11.0   \n",
       "453  509         1   63    52.0   164.0  19.333730       60.556     NaN   \n",
       "454  510         1   26    85.0   172.0  28.731747       67.836    31.0   \n",
       "455  511         1   78    60.0   158.0  24.034610       55.096    28.0   \n",
       "\n",
       "       PF_ratio  SOFA_TOT  ...  NonEFL_LeftFromParallelCriteria  \\\n",
       "0    730.000000         5  ...                              NaN   \n",
       "1    604.000000         0  ...                              NaN   \n",
       "2    365.000000         2  ...                              NaN   \n",
       "3    342.000000         1  ...                              NaN   \n",
       "4    257.000000         5  ...                              NaN   \n",
       "..          ...       ...  ...                              ...   \n",
       "451  326.666667         1  ...                              NaN   \n",
       "452  546.666667         0  ...                              NaN   \n",
       "453  640.000000         0  ...                              NaN   \n",
       "454  167.500000        14  ...                              NaN   \n",
       "455  506.666667         4  ...                              NaN   \n",
       "\n",
       "    NewEFL_type_parallel Type2_removed  RelativeEFL_DeltaVt_Less20  \\\n",
       "0                    NaN           NaN                         NaN   \n",
       "1                    NaN           NaN                         NaN   \n",
       "2                    NaN           NaN                         NaN   \n",
       "3                    NaN           NaN                         NaN   \n",
       "4                    NaN           NaN                         NaN   \n",
       "..                   ...           ...                         ...   \n",
       "451                  NaN           NaN                         NaN   \n",
       "452                  NaN           NaN                         NaN   \n",
       "453                  NaN           NaN                         NaN   \n",
       "454                  NaN           NaN                         NaN   \n",
       "455                  NaN           NaN                         NaN   \n",
       "\n",
       "     NonEFL_LeftFromDeltaVtCriteria  NewEFL_type_deltaVt  DefiniteEFL_nonEFL  \\\n",
       "0                               NaN                  NaN                 NaN   \n",
       "1                               NaN                  NaN                 NaN   \n",
       "2                               NaN                  NaN                 NaN   \n",
       "3                               NaN                  NaN                 NaN   \n",
       "4                               NaN                  NaN                 NaN   \n",
       "..                              ...                  ...                 ...   \n",
       "451                             NaN                  NaN                 NaN   \n",
       "452                             NaN                  NaN                 NaN   \n",
       "453                             NaN                  NaN                 NaN   \n",
       "454                             NaN                  NaN                 NaN   \n",
       "455                             NaN                  NaN                 NaN   \n",
       "\n",
       "     EFL_by_Standardized Criteria EFL_by_NewlyProposed_RexAnalysis  \\\n",
       "0                             0.0                              0.0   \n",
       "1                             0.0                              0.0   \n",
       "2                             0.0                              0.0   \n",
       "3                             0.0                              0.0   \n",
       "4                             NaN                              0.0   \n",
       "..                            ...                              ...   \n",
       "451                           0.0                              0.0   \n",
       "452                           0.0                              0.0   \n",
       "453                           0.0                              0.0   \n",
       "454                           0.0                              0.0   \n",
       "455                           0.0                              0.0   \n",
       "\n",
       "    EFL_by_mixed criteria  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "..                    ...  \n",
       "451                   0.0  \n",
       "452                   0.0  \n",
       "453                   0.0  \n",
       "454                   0.0  \n",
       "455                   0.0  \n",
       "\n",
       "[455 rows x 37 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##master = pd.read_excel('F:\\junwei\\dataset\\_MASTER_MAFAI2023_456cases.xlsx')\n",
    "# List of IDs to remove\n",
    "#ids_to_remove = [ 60, 33, 74, 24, 19, 48, 30, 174, 108, 138, 109, 179, 144, 123, 206, 254, 381, 321]\n",
    "\n",
    "# Remove the rows with the specified IDs\n",
    "#master1 = master[~master['ID'].isin(ids_to_remove)]\n",
    "\n",
    "#master1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers from ID column: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 139, 140, 141, 142, 143, 145, 147, 148, 149, 150, 151, 152, 153, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 256, 257, 259, 260, 262, 264, 265, 266, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 382, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 471, 472, 473, 474, 475, 476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n"
     ]
    }
   ],
   "source": [
    "#numbers = master1['ID'].tolist()\n",
    "\n",
    "#print(\"Numbers from ID column:\", numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing IDs: 55\n",
      "List of missing IDs: [24, 30, 33, 41, 48, 56, 60, 61, 74, 81, 93, 100, 101, 108, 109, 113, 123, 124, 131, 137, 138, 144, 146, 154, 159, 171, 174, 179, 206, 209, 213, 229, 233, 250, 254, 255, 258, 261, 263, 267, 288, 293, 315, 321, 332, 336, 381, 383, 390, 403, 421, 428, 443, 470, 477]\n"
     ]
    }
   ],
   "source": [
    "# Find missing IDs\n",
    "#expected_ids = set(range(1, 512))\n",
    "#actual_ids = set(master['ID'])\n",
    "#missing_ids = sorted(list(expected_ids - actual_ids))\n",
    "\n",
    "# Count missing IDs\n",
    "#num_missing = len(missing_ids)\n",
    "#print(f\"Number of missing IDs: {num_missing}\")\n",
    "#print(f\"List of missing IDs: {missing_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique numbers after MV: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "#folder_path = 'F:\\junwei\\dataset\\MV001-100'\n",
    "#unique_numbers1 = get_unique_numbers_after_mv(folder_path)\n",
    "#print(\"Unique numbers after MV:\", unique_numbers1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique numbers after MV: [102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 139, 140, 141, 142, 143, 145, 147, 148, 149, 150, 151, 152, 153, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "#folder_path2 = 'F:\\junwei\\dataset\\MV101-200'\n",
    "#unique_numbers2 = get_unique_numbers_after_mv(folder_path2)\n",
    "#print(\"Unique numbers after MV:\", unique_numbers2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique numbers after MV: [201, 202, 203, 204, 205, 207, 208, 210, 211, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 230, 231, 232, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 252, 253, 256, 257, 259, 260, 262, 264, 265, 266, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#folder_path3 = 'F:\\junwei\\dataset\\MV201-300'\n",
    "#unique_numbers3 = get_unique_numbers_after_mv(folder_path3)\n",
    "#print(\"Unique numbers after MV:\", unique_numbers3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique numbers after MV: [301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 382, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400]\n"
     ]
    }
   ],
   "source": [
    "#folder_path4 = 'F:\\junwei\\dataset\\MV301-400'\n",
    "#unique_numbers4 = get_unique_numbers_after_mv(folder_path4)\n",
    "#print(\"Unique numbers after MV:\", unique_numbers4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique numbers after MV: [401, 402, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 426, 427, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 471, 472, 473, 474, 475, 476, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511]\n"
     ]
    }
   ],
   "source": [
    "#folder_path5 = 'F:\\junwei\\dataset\\MV401-511'\n",
    "#unique_numbers5 = get_unique_numbers_after_mv(folder_path5)\n",
    "#print(\"Unique numbers after MV:\", unique_numbers5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_unique_numbers = []\n",
    "\n",
    "#combined_unique_numbers.extend(unique_numbers1)\n",
    "#combined_unique_numbers.extend(unique_numbers2)\n",
    "#combined_unique_numbers.extend(unique_numbers3)\n",
    "#combined_unique_numbers.extend(unique_numbers4)\n",
    "#combined_unique_numbers.extend(unique_numbers5)\n",
    "\n",
    "#combined_unique_numbers = list(set(combined_unique_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique data that only occurs in one of the lists: [19]\n"
     ]
    }
   ],
   "source": [
    "#symmetric_difference = list(set(numbers) ^ set(combined_unique_numbers))\n",
    "\n",
    "#print(\"Unique data that only occurs in one of the lists:\", symmetric_difference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
