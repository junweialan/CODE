{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import stft\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nbimporter\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error,r2_score\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import optuna\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, nhead, num_layers, output_dim, device, dropout_p,hidden_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout_p, max_seq_len)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "\n",
    "        layers = []\n",
    "        in_dim = d_model\n",
    "        for out_dim in hidden_layers:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "                nn.Dropout(dropout_p),\n",
    "                nn.LeakyReLU()\n",
    "            ])\n",
    "            in_dim = out_dim\n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.regressor = nn.Sequential(*layers)\n",
    " \n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1) \n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "    \n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, num_epochs, patience, device, mse_loss, optimizer):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        self.mse_loss = mse_loss\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        self.consecutive_greater_count = 0\n",
    "        self.min_valid_loss = float('inf')\n",
    "        \n",
    "   # self.mse_loss = mse_loss\n",
    "    # Training loop\n",
    "    def train(self):\n",
    "        self.consecutive_greater_count = 0 \n",
    "        self.min_valid_loss = float('inf')\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for inputs, targets in self.train_loader:\n",
    "            \n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                loss = self.mse_loss(outputs, targets)  #self.mse_loss\n",
    "\n",
    "               \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(self.train_loader)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            valid_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in self.valid_loader:\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    \n",
    "                    outputs = self.model(inputs)\n",
    "\n",
    "                    \n",
    "                    loss = self.mse_loss(outputs, targets) #self.mse_loss\n",
    "\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                valid_loss /= len(self.valid_loader)\n",
    "\n",
    "            if math.isnan(valid_loss):\n",
    "                print(\"Validation loss is NaN. Stopping training for this trial.\")\n",
    "                self.valid_losses.append(float('inf'))\n",
    "                break\n",
    "            \n",
    "            #print(f'Epoch [{epoch+1}/{self.num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.valid_losses.append(valid_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if valid_loss < self.min_valid_loss:\n",
    "                self.min_valid_loss = valid_loss\n",
    "                self.consecutive_greater_count = 0\n",
    "                \n",
    "            elif valid_loss == self.min_valid_loss:  # Check if the validation loss is the same as the minimum\n",
    "                self.consecutive_greater_count += 1\n",
    "                if self.consecutive_greater_count >= self.patience:\n",
    "                    print(f'Stopping training after {epoch+1} epochs, validation loss not improving')\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                self.consecutive_greater_count += 1\n",
    "            if self.consecutive_greater_count == self.patience:\n",
    "                # Save the best model \n",
    "                #torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "            #self.scheduler.step()\n",
    "        return self.train_losses, self.valid_losses, self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPiece:\n",
    "    def __init__(self, vocab_size, max_seq_len):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = Tokenizer(models.WordPiece())\n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        self.tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "    def train_tokenizer(self, data):\n",
    "        data_as_strings = [' '.join(sequence.astype(str)) for sequence in data]\n",
    "        trainer = trainers.WordPieceTrainer(vocab_size=self.vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "        self.tokenizer.train_from_iterator(data_as_strings, trainer=trainer)\n",
    "\n",
    "    def tokenize_and_pad(self, data):\n",
    "        data_as_strings = [' '.join(sequence.astype(str)) for sequence in data]\n",
    "        tokenized_data = [self.tokenizer.encode(sequence).ids for sequence in data_as_strings]\n",
    "        padded_data = pad_sequence(\n",
    "            [torch.tensor(seq) for seq in tokenized_data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.token_to_id(\"[PAD]\")\n",
    "        ).numpy()\n",
    "        return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEtokenizer:\n",
    "    def __init__(self, vocab_size, max_seq_len):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tokenizer = Tokenizer(models.BPE())\n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        self.tokenizer.decoder = decoders.BPEDecoder()\n",
    "\n",
    "    def train_tokenizer(self, data):\n",
    "        data_as_strings = [' '.join(sequence.astype(str)) for sequence in data]\n",
    "        trainer = trainers.BpeTrainer(vocab_size=self.vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "        self.tokenizer.train_from_iterator(data_as_strings, trainer=trainer)\n",
    "\n",
    "    def tokenize_and_pad(self, data):\n",
    "        data_as_strings = [' '.join(sequence.astype(str)) for sequence in data]\n",
    "        tokenized_data = [self.tokenizer.encode(sequence).ids for sequence in data_as_strings]\n",
    "        padded_data = pad_sequence(\n",
    "            [torch.tensor(seq) for seq in tokenized_data],\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.token_to_id(\"[PAD]\")\n",
    "        ).numpy()\n",
    "        return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "class Evaluator:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "    def normalized_mean_squared_error(self):\n",
    "        err_pmus = np.array(self.y_true)\n",
    "        err_pmus_hat = np.array(self.y_pred)\n",
    "        nrmse = np.sqrt(np.sum((err_pmus - err_pmus_hat)**2))/np.sqrt(np.sum((err_pmus - np.average(err_pmus))**2))\n",
    "        return nrmse\n",
    "    \n",
    "    def pearson_r(self):\n",
    "        self.y_true = np.squeeze(self.y_true)\n",
    "        self.y_pred = np.squeeze(self.y_pred)\n",
    "        pearson_r, _ = pearsonr(self.y_true, self.y_pred)\n",
    "\n",
    "        return pearson_r\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        mse = mean_squared_error(self.y_true, self.y_pred)\n",
    "        mae = mean_absolute_error(self.y_true, self.y_pred)\n",
    "        nmse = self.normalized_mean_squared_error()\n",
    "        pearson_r = self.pearson_r()\n",
    "        mape = mean_absolute_percentage_error(self.y_true, self.y_pred)*100\n",
    "\n",
    "        return mse, mae, nmse, mape, pearson_r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlandAltman2:\n",
    "    def __init__(self, output_data_unscaled, pre_data_unscaled):\n",
    "        self.output_data_unscaled = output_data_unscaled\n",
    "        self.pre_data_unscaled = pre_data_unscaled\n",
    "\n",
    "    def plot(self):\n",
    "       \n",
    "        output_data = np.asarray(self.output_data_unscaled)\n",
    "        pre_data = np.asarray(self.pre_data_unscaled)\n",
    "\n",
    "      \n",
    "        diff = (output_data - pre_data) / output_data * 100\n",
    "\n",
    "        \n",
    "        average_compliance = (output_data + pre_data) / 2\n",
    "\n",
    "      \n",
    "        mean_diff = np.mean(diff)\n",
    "        std_diff = np.std(diff)\n",
    "\n",
    "        overestimate_indices = []\n",
    "        underestimate_indices = []\n",
    "        # Find indices of outliers\n",
    "        outlier_indices = []\n",
    "\n",
    "        for i, percentage_error in enumerate(diff):\n",
    "            if percentage_error > 20:  # overestimate\n",
    "                outlier_indices.append(i)\n",
    "                overestimate_indices.append(i)\n",
    "            elif percentage_error < -20:  # underestimate\n",
    "                outlier_indices.append(i)\n",
    "                underestimate_indices.append(i)\n",
    "\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for i in range(len(diff)):\n",
    "            if i in outlier_indices:\n",
    "                ax.scatter(average_compliance[i], diff[i], color='blue', s=10)\n",
    "            else:\n",
    "                ax.scatter(average_compliance[i], diff[i], color='blue', s=10)\n",
    "        ax.axhline(mean_diff, color='gray', linestyle='--')\n",
    "        ax.axhline(mean_diff + 1.96 * std_diff, color='gray', linestyle='--')\n",
    "        ax.axhline(mean_diff - 1.96 * std_diff, color='gray', linestyle='--')\n",
    "        ax.set_xlabel('Average Compliance Value')\n",
    "        ax.set_ylabel('Percentage Difference between Predictions and Targets')\n",
    "        ax.set_title('Bland-Altman Plot')\n",
    "        plt.show()\n",
    "\n",
    "        # Print indices of outliers\n",
    "        print(\"Indices of outliers: \", outlier_indices)\n",
    "        print(\"Indices of overestimations: \", overestimate_indices)\n",
    "        print(\"Indices of underestimations: \", underestimate_indices)\n",
    "        return outlier_indices, overestimate_indices, underestimate_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlandAltman1:\n",
    "    def __init__(self, output_data_unscaled, pre_data_unscaled, test_indices):\n",
    "        self.output_data_unscaled = output_data_unscaled\n",
    "        self.pre_data_unscaled = pre_data_unscaled\n",
    "        self.test_indices = test_indices\n",
    "    \n",
    "    def plot(self):\n",
    "        \n",
    "        diff = (np.asarray(self.output_data_unscaled) - np.asarray(self.pre_data_unscaled)) / np.asarray(self.output_data_unscaled) * 100\n",
    "\n",
    "     \n",
    "        mean_diff = np.mean(diff)\n",
    "        std_diff = np.std(diff)\n",
    "        overestimate_indices = []\n",
    "        underestimate_indices = []\n",
    "       \n",
    "        outlier_indices = []\n",
    "        \n",
    "   \n",
    "        for i, percentage_error in enumerate(diff):\n",
    "            if percentage_error > 20:  # overestimate\n",
    "                outlier_indices.append(i)\n",
    "                overestimate_indices.append(i)\n",
    "            elif percentage_error < -20:  # underestimate\n",
    "                outlier_indices.append(i)\n",
    "                underestimate_indices.append(i)\n",
    "\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for i in range(len(diff)):\n",
    "            if i in outlier_indices:\n",
    "                ax.scatter(np.asarray(self.output_data_unscaled)[i], diff[i], color='red', s=10)\n",
    "                ax.text(np.asarray(self.output_data_unscaled)[i], diff[i], str(self.test_indices[i]))\n",
    "            else:\n",
    "                ax.scatter(np.asarray(self.output_data_unscaled)[i], diff[i], color='blue', s=10)\n",
    "        ax.axhline(mean_diff, color='gray', linestyle='--')\n",
    "        ax.axhline(mean_diff + 1.96 * std_diff, color='gray', linestyle='--')\n",
    "        ax.axhline(mean_diff - 1.96 * std_diff, color='gray', linestyle='--')\n",
    "        ax.set_xlabel('Target')\n",
    "        ax.set_ylabel('Percentage Difference between Predictions and Targets')\n",
    "        ax.set_title('Bland-Altman Plot')\n",
    "        plt.show()\n",
    "\n",
    "        # Print indices of outliers\n",
    "        print(\"Indices of outliers: \", [self.test_indices[i] for i in outlier_indices])\n",
    "        print(\"Indices of overestimations: \", [self.test_indices[i] for i in overestimate_indices])\n",
    "        print(\"Indices of underestimations: \", [self.test_indices[i] for i in underestimate_indices])\n",
    "        return outlier_indices, overestimate_indices, underestimate_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlandAltman:\n",
    "    def __init__(self, output_data_unscaled, pre_data_unscaled, test_indices):\n",
    "        self.output_data_unscaled = output_data_unscaled\n",
    "        self.pre_data_unscaled = pre_data_unscaled\n",
    "        self.test_indices = test_indices\n",
    "    def plot(self):\n",
    "       \n",
    "        output_data = np.asarray(self.output_data_unscaled)\n",
    "        pre_data = np.asarray(self.pre_data_unscaled)\n",
    "\n",
    "     \n",
    "        diff = (output_data - pre_data) / output_data * 100\n",
    "\n",
    "      \n",
    "        average_compliance = (output_data + pre_data) / 2\n",
    "\n",
    "      \n",
    "        mean_diff = np.mean(diff)\n",
    "        std_diff = np.std(diff)\n",
    "\n",
    "        overestimate_indices = []\n",
    "        underestimate_indices = []\n",
    "       \n",
    "        outlier_indices = []\n",
    "\n",
    "        for i, percentage_error in enumerate(diff):\n",
    "            if percentage_error > 20:  # overestimate\n",
    "                outlier_indices.append(i)\n",
    "                overestimate_indices.append(i)\n",
    "            elif percentage_error < -20:  # underestimate\n",
    "                outlier_indices.append(i)\n",
    "                underestimate_indices.append(i)\n",
    "\n",
    "     \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        for i in range(len(diff)):\n",
    "            if i in outlier_indices:\n",
    "                ax.scatter(average_compliance[i], diff[i], color='red', s=10)\n",
    "                ax.text(average_compliance[i], diff[i], str(self.test_indices[i]))\n",
    "            else:\n",
    "                ax.scatter(average_compliance[i], diff[i], color='blue', s=10)\n",
    "        ax.axhline(mean_diff, color='gray', linestyle='--')\n",
    "        ax.axhline(mean_diff + 1.96 * std_diff, color='gray', linestyle='--')\n",
    "        ax.axhline(mean_diff - 1.96 * std_diff, color='gray', linestyle='--')\n",
    "        ax.set_xlabel('Average Compliance Value')\n",
    "        ax.set_ylabel('Percentage Difference between Predictions and Targets')\n",
    "        ax.set_title('Bland-Altman Plot')\n",
    "        plt.show()\n",
    "\n",
    "        # Print indices of outliers\n",
    "        print(\"Indices of outliers: \", [self.test_indices[i] for i in outlier_indices])\n",
    "        print(\"Indices of overestimations: \", [self.test_indices[i] for i in overestimate_indices])\n",
    "        print(\"Indices of underestimations: \", [self.test_indices[i] for i in underestimate_indices])\n",
    "        return outlier_indices, overestimate_indices, underestimate_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "class Objective:\n",
    "    def __init__(self, train_loader, valid_loader, device, d_model_range, nhead_range, num_layers_range, dropout_range, lr_range, n_trials, vocab_size, max_seq_len, output_dim, num_epochs, patience, weight_decay,num_hidden_layers_range,hidden_dim_range):\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.device = device\n",
    "        self.d_model_range = d_model_range\n",
    "        self.nhead_range = nhead_range\n",
    "        self.num_layers_range = num_layers_range\n",
    "        self.dropout_range = dropout_range\n",
    "        self.lr_range = lr_range\n",
    "        self.n_trials = n_trials\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.output_dim = output_dim\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.weight_decay = weight_decay\n",
    "        self.trial_num = 1\n",
    "        self.best_valid_loss = float('inf')\n",
    "        self.num_hidden_layers_range = num_hidden_layers_range\n",
    "        self.hidden_dim_range = hidden_dim_range\n",
    "        self.all_train_losses = []\n",
    "        self.all_valid_losses = []\n",
    "        \n",
    "        self.trial_num = 1\n",
    "\n",
    "    def save_trial_data(self, trial_num, params, model, train_losses, valid_losses):\n",
    "        \n",
    "\n",
    "        all_trials_folder_path = r'F:\\junwei\\final_tranformer\\all_trails_(10HZ-800)_C_R'\n",
    "        if not os.path.exists(all_trials_folder_path):\n",
    "            os.makedirs(all_trials_folder_path)\n",
    "\n",
    "        with open(os.path.join(all_trials_folder_path, f'trial_{trial_num}.pkl'), 'wb') as f:\n",
    "            pickle.dump((model.state_dict(), params, train_losses, valid_losses), f)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        trial.params['d_model'] = trial.suggest_categorical('d_model', self.d_model_range)\n",
    "        trial.params['nhead'] = trial.suggest_categorical('nhead', self.nhead_range)\n",
    "        trial.params['weight_decay'] = trial.suggest_loguniform(\"weight_decay\", *self.weight_decay)\n",
    "        trial.params['num_layers'] = trial.suggest_int(\"num_layers\", *self.num_layers_range)\n",
    "        trial.params['dropout'] = trial.suggest_float(\"dropout\", *self.dropout_range)\n",
    "        trial.params['lr'] = trial.suggest_loguniform(\"lr\", *self.lr_range)\n",
    "        num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", *self.num_hidden_layers_range)\n",
    "        hidden_layers = []\n",
    "        for i in range(num_hidden_layers):\n",
    "            hidden_dim = trial.suggest_categorical(f\"hidden_dim_{i}\", self.hidden_dim_range)\n",
    "            hidden_layers.append(hidden_dim)\n",
    "\n",
    "\n",
    "        model = Transformer(self.vocab_size, self.max_seq_len, trial.params['d_model'], trial.params['nhead'], trial.params['num_layers'], self.output_dim, self.device, trial.params['dropout'], hidden_layers).to(self.device)\n",
    "        mse_loss = nn.L1Loss()\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=trial.params['lr'], weight_decay=trial.params['weight_decay'])\n",
    "\n",
    "        trainer = Trainer(model=model, \n",
    "                          train_loader=self.train_loader, \n",
    "                          valid_loader=self.valid_loader, \n",
    "                          num_epochs=self.num_epochs, \n",
    "                          patience=self.patience, \n",
    "                          device=self.device, \n",
    "                          mse_loss=mse_loss, \n",
    "                          optimizer=optimizer\n",
    "                          )\n",
    "\n",
    "        train_losses, valid_losses, _ = trainer.train()\n",
    "        self.all_train_losses.append(train_losses)\n",
    "        self.all_valid_losses.append(valid_losses)\n",
    "\n",
    "        params = trial.params\n",
    "        params['d_model'] = trial.params['d_model']\n",
    "        params['nhead'] = trial.params['nhead']\n",
    "        params['hidden_layers'] = [trial.params[f'hidden_dim_{i}'] for i in range(trial.params['num_hidden_layers'])]\n",
    "\n",
    "        self.save_trial_data(self.trial_num, params, model, train_losses, valid_losses)\n",
    "        \n",
    "\n",
    "\n",
    "        if valid_losses[-1] < self.best_valid_loss:\n",
    "            self.best_valid_loss = valid_losses[-1]\n",
    "            self.best_model = model\n",
    "            self.best_train_losses = train_losses\n",
    "            self.best_valid_losses = valid_losses\n",
    "\n",
    "        self.trial_num += 1\n",
    "        return valid_losses[-1]  # Return the last validation loss\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        for i in range(self.n_trials):\n",
    "            plt.plot(self.all_train_losses[i], label='Train Loss')\n",
    "            plt.plot(self.all_valid_losses[i], label='Validation Loss')\n",
    "            plt.title(f'Trial {i+1}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def optimize(self):\n",
    "        sampler = optuna.samplers.TPESampler(seed=37)\n",
    "        study = optuna.create_study(sampler = sampler, direction=\"minimize\")\n",
    "        study.optimize(self, n_trials=self.n_trials, n_jobs=1)\n",
    "        best_trial = study.best_trial\n",
    "\n",
    "        best_params = best_trial.params\n",
    "        best_params['d_model'] = best_trial.params['d_model']\n",
    "        best_params['nhead'] = best_trial.params['nhead']\n",
    "        best_params['hidden_layers'] = [best_trial.params[f'hidden_dim_{i}'] for i in range(best_trial.params['num_hidden_layers'])]\n",
    "\n",
    "        folder_path = r'F:\\junwei\\final_tranformer\\best_model_C_R(10HZ-800)'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    \n",
    "\n",
    "        with open(os.path.join(folder_path, 'best_model_and_params.pkl'), 'wb') as f:\n",
    "            pickle.dump((self.best_model.state_dict(), best_params, self.best_train_losses, self.best_valid_losses), f)\n",
    "\n",
    "            \n",
    "        fig = plot_param_importances(study)\n",
    "        fig.show()\n",
    "\n",
    "        plt.plot(self.best_train_losses, label='Training Loss')\n",
    "        plt.plot(self.best_valid_losses, label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return (best_trial.value, best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModelLoader:\n",
    "    def __init__(self, path, vocab_size, max_seq_len, output_dim, device):\n",
    "        self.path = path\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "\n",
    "    def load_best_model(self):\n",
    "   \n",
    "        with open(self.path, 'rb') as f:\n",
    "            best_model_state_dict, best_params, best_train_losses, best_valid_losses = pickle.load(f)\n",
    "\n",
    "\n",
    "     \n",
    "        model = Transformer(self.vocab_size, self.max_seq_len, best_params['d_model'], best_params['nhead'], best_params['num_layers'], self.output_dim, self.device, best_params['dropout'], best_params['hidden_layers']).to(self.device)\n",
    "\n",
    "     \n",
    "        best_model_state_dict = {k: v.to(self.device) for k, v in best_model_state_dict.items()}\n",
    "\n",
    "  \n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "        return model,best_train_losses, best_valid_losses\n",
    "    \n",
    "    def plot_loss(self, best_train_losses, best_valid_losses):\n",
    "        # Plot the train and validation losses\n",
    "        plt.plot(best_train_losses, label='Training Loss')\n",
    "        plt.plot(best_valid_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def test_model(self, model, test_loader):\n",
    "        model.eval()\n",
    "        num_samples = 0\n",
    "        preds = []\n",
    "        targets_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                targets = targets.cpu().numpy()\n",
    "\n",
    "                num_samples += len(targets)\n",
    "                preds.append(outputs)\n",
    "                targets_list.append(targets)\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        targets = np.concatenate(targets_list, axis=0)\n",
    "        \n",
    "        return preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data1(data):\n",
    "    minimum = np.min(data, axis=(0, 1), keepdims=True)\n",
    "    maximum = np.max(data, axis=(0, 1), keepdims=True)\n",
    "    data_norm = (data - minimum) / (maximum - minimum)\n",
    "    return minimum, maximum, data_norm\n",
    "\n",
    "def normalize_data2(data, minimum = None,maximum = None):\n",
    "    if minimum is None:\n",
    "        minimum = np.min(np.min(data))\n",
    "    if maximum is None:\n",
    "        maximum = np.max(np.max(data))\n",
    "    data_norm = (data - minimum) / (maximum - minimum)\n",
    "    return minimum, maximum, data_norm\n",
    "\n",
    "def denormalize_data(data, minimum, maximum):\n",
    "    return data * (maximum - minimum) + minimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter(true,pre,name):   \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(true, pre, c='blue',s=10)\n",
    "\n",
    "    lims = [\n",
    "        np.min([plt.xlim(), plt.ylim()]),  # min of both axes\n",
    "        np.max([plt.xlim(), plt.ylim()]),  # max of both axes\n",
    "    ]\n",
    "    plt.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "    plt.plot(lims, [lims[0]*0.8, lims[1]*0.8], 'r-', alpha=0.5)  \n",
    "    plt.plot(lims, [lims[0]*1.2, lims[1]*1.2], 'r-', alpha=0.5)  \n",
    "\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(name + ' True vs Predicted Values')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ids(index_list, test_data1, dataset1):\n",
    "    id_list = []\n",
    "    for i in index_list:\n",
    "        if i in test_data1.index:\n",
    "            id_value = test_data1.loc[i, 'ID']\n",
    "            id_list.append(id_value)\n",
    "    dataset2 = dataset1[dataset1['ID'].isin(id_list)]\n",
    "    return dataset2 , id_list "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Junwei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
